{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3984a2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.20.0\n",
      "âš ï¸ No GPU detected. Training will run on CPU (Slower).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Check GPU\n",
    "print(\"TensorFlow Version:\", tf.__version__)\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"âœ… GPU Detected: {gpus}\")\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"ğŸš€ Memory growth enabled for GPU\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"âš ï¸ No GPU detected. Training will run on CPU (Slower).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "346de0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIGURATION ---\n",
    "DATA_FILE = 'C:\\\\Users\\\\adith\\\\Downloads\\\\TradingProject\\\\data\\\\BTC_USD.csv'\n",
    "MODEL_DIR = 'models'\n",
    "MODEL_FILE = f'{MODEL_DIR}/market_model.h5'\n",
    "SCALER_FILE = f'{MODEL_DIR}/market_scaler.pkl'\n",
    "LOOKBACK = 60\n",
    "TEST_SPLIT = 0.2\n",
    "\n",
    "# Create models folder if it doesn't exist\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.makedirs(MODEL_DIR)\n",
    "    print(f\"Created directory: {MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9c0fd86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§  Training features: ['BTC_Close', 'BTC_Volume']\n",
      "âš–ï¸  Scaling Multi-Variate Data...\n",
      "âœ… Data Loaded & Scaled.\n"
     ]
    }
   ],
   "source": [
    "# 1. Load Data\n",
    "if not os.path.exists(DATA_FILE):\n",
    "    raise FileNotFoundError(f\"âŒ Error: {DATA_FILE} not found. Run merge_data.py first!\")\n",
    "\n",
    "df = pd.read_csv(DATA_FILE)\n",
    "\n",
    "# 2. Select Features\n",
    "# We use Close Price and Volume for both BTC and ETH\n",
    "feature_cols = ['BTC_Close', 'BTC_Volume']\n",
    "print(f\"ğŸ§  Training features: {feature_cols}\")\n",
    "\n",
    "dataset = df[feature_cols].values\n",
    "\n",
    "# 3. Scale Data (0 to 1)\n",
    "print(\"âš–ï¸  Scaling Multi-Variate Data...\")\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(dataset)\n",
    "\n",
    "print(\"âœ… Data Loaded & Scaled.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac581a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Creating sequences...\n",
      "âœ… Data Prepared. Input Shape: (3220, 60, 2)\n"
     ]
    }
   ],
   "source": [
    "# Create Sequences\n",
    "print(\"ğŸ”„ Creating sequences...\")\n",
    "X, y = [], []\n",
    "\n",
    "for i in range(LOOKBACK, len(scaled_data)):\n",
    "    # Input: Past 60 days of ALL features\n",
    "    X.append(scaled_data[i-LOOKBACK:i]) \n",
    "    # Target: Day 61 of BTC Price (Column 0)\n",
    "    y.append(scaled_data[i, 0]) \n",
    "    \n",
    "X, y = np.array(X), np.array(y)\n",
    "\n",
    "# Split Train/Test\n",
    "split_idx = int(len(X) * (1 - TEST_SPLIT))\n",
    "X_train, X_val = X[:split_idx], X[split_idx:]\n",
    "y_train, y_val = y[:split_idx], y[split_idx:]\n",
    "\n",
    "print(f\"âœ… Data Prepared. Input Shape: {X_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d5ab3c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§± Building Neural Network...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\adith\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">17,152</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              â”‚            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m64\u001b[0m)         â”‚        \u001b[38;5;34m17,152\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (\u001b[38;5;33mDropout\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m64\u001b[0m)         â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             â”‚        \u001b[38;5;34m33,024\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (\u001b[38;5;33mDense\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              â”‚            \u001b[38;5;34m65\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">50,241</span> (196.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m50,241\u001b[0m (196.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">50,241</span> (196.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m50,241\u001b[0m (196.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build Model\n",
    "print(f\"ğŸ§± Building Neural Network...\")\n",
    "model = Sequential()\n",
    "\n",
    "# Layer 1\n",
    "model.add(LSTM(64, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Layer 2\n",
    "model.add(LSTM(64, return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Output Layer\n",
    "model.add(Dense(1)) # Predict BTC Price\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "663e50c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Starting Training...\n",
      "Epoch 1/50\n",
      "\u001b[1m101/101\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0049\n",
      "Epoch 1: val_loss improved from None to 0.00109, saving model to models/market_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m101/101\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 34ms/step - loss: 0.0018 - val_loss: 0.0011\n",
      "Epoch 2/50\n",
      "\u001b[1m101/101\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 4.4691e-04\n",
      "Epoch 2: val_loss did not improve from 0.00109\n",
      "\u001b[1m101/101\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - loss: 4.5214e-04 - val_loss: 0.0059\n",
      "Epoch 3/50\n",
      "\u001b[1m100/101\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 5.0016e-04\n",
      "Epoch 3: val_loss did not improve from 0.00109\n",
      "\u001b[1m101/101\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - loss: 4.4814e-04 - val_loss: 0.0019\n",
      "Epoch 4/50\n",
      "\u001b[1m101/101\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 3.3984e-04\n",
      "Epoch 4: val_loss did not improve from 0.00109\n",
      "\u001b[1m101/101\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - loss: 3.5669e-04 - val_loss: 0.0026\n",
      "Epoch 5/50\n",
      "\u001b[1m101/101\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 3.4097e-04\n",
      "Epoch 5: val_loss improved from 0.00109 to 0.00074, saving model to models/market_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m101/101\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 38ms/step - loss: 3.2950e-04 - val_loss: 7.3515e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m100/101\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 3.5503e-04\n",
      "Epoch 6: val_loss did not improve from 0.00074\n",
      "\u001b[1m101/101\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - loss: 3.2057e-04 - val_loss: 7.6024e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m101/101\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 3.3387e-04\n",
      "Epoch 7: val_loss did not improve from 0.00074\n",
      "\u001b[1m101/101\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 38ms/step - loss: 3.3947e-04 - val_loss: 0.0013\n",
      "Epoch 8/50\n",
      "\u001b[1m101/101\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 3.1877e-04\n",
      "Epoch 8: val_loss did not improve from 0.00074\n",
      "\u001b[1m101/101\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 35ms/step - loss: 3.1119e-04 - val_loss: 0.0018\n",
      "Epoch 9/50\n",
      "\u001b[1m101/101\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 2.6474e-04\n",
      "Epoch 9: val_loss improved from 0.00074 to 0.00060, saving model to models/market_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m101/101\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - loss: 2.9210e-04 - val_loss: 6.0055e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m100/101\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 2.3922e-04\n",
      "Epoch 10: val_loss did not improve from 0.00060\n",
      "\u001b[1m101/101\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 28ms/step - loss: 2.4307e-04 - val_loss: 0.0011\n",
      "Epoch 11/50\n",
      "\u001b[1m101/101\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 2.5560e-04\n",
      "Epoch 11: val_loss did not improve from 0.00060\n",
      "\u001b[1m101/101\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 29ms/step - loss: 2.6351e-04 - val_loss: 0.0011\n",
      "Epoch 12/50\n",
      "\u001b[1m101/101\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 2.6568e-04\n",
      "Epoch 12: val_loss did not improve from 0.00060\n",
      "\u001b[1m101/101\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - loss: 2.6419e-04 - val_loss: 0.0028\n",
      "Epoch 13/50\n",
      "\u001b[1m101/101\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 2.3843e-04\n",
      "Epoch 13: val_loss improved from 0.00060 to 0.00051, saving model to models/market_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m101/101\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 35ms/step - loss: 2.3132e-04 - val_loss: 5.1461e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m101/101\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 2.1643e-04\n",
      "Epoch 14: val_loss did not improve from 0.00051\n",
      "\u001b[1m101/101\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - loss: 2.1005e-04 - val_loss: 0.0017\n",
      "Epoch 15/50\n",
      "\u001b[1m101/101\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 2.5052e-04\n",
      "Epoch 15: val_loss did not improve from 0.00051\n",
      "\u001b[1m101/101\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - loss: 2.3809e-04 - val_loss: 0.0012\n",
      "Epoch 16/50\n",
      "\u001b[1m101/101\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 1.9943e-04\n",
      "Epoch 16: val_loss did not improve from 0.00051\n",
      "\u001b[1m101/101\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - loss: 2.1280e-04 - val_loss: 9.6635e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m101/101\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 2.2302e-04\n",
      "Epoch 17: val_loss did not improve from 0.00051\n",
      "\u001b[1m101/101\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 35ms/step - loss: 2.2003e-04 - val_loss: 0.0020\n",
      "Epoch 18/50\n",
      "\u001b[1m101/101\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 2.1799e-04\n",
      "Epoch 18: val_loss did not improve from 0.00051\n",
      "\u001b[1m101/101\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - loss: 2.1568e-04 - val_loss: 0.0012\n",
      "Epoch 18: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=5, verbose=1),\n",
    "    ModelCheckpoint(MODEL_FILE, monitor='val_loss', save_best_only=True, verbose=1)\n",
    "]\n",
    "\n",
    "print(\"ğŸš€ Starting Training...\")\n",
    "history = model.fit(\n",
    "    X_train, y_train, \n",
    "    epochs=50, \n",
    "    batch_size=32, \n",
    "    validation_data=(X_val, y_val), \n",
    "    callbacks=callbacks, \n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "70b5dfc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scaler Saved!\n",
      "ğŸ‰ Process Complete. Files ready in 'models' folder.\n"
     ]
    }
   ],
   "source": [
    "# Save Scaler\n",
    "pickle.dump(scaler, open(SCALER_FILE, \"wb\"))\n",
    "print(\"âœ… Scaler Saved!\")\n",
    "print(f\"ğŸ‰ Process Complete. Files ready in '{MODEL_DIR}' folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6f5e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Loading Resources...\n",
      "ğŸ“Š Evaluating on 806 unseen data points...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (806,4) (2,) (806,4) ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 55\u001b[39m\n\u001b[32m     52\u001b[39m dummy_pred[:, \u001b[32m0\u001b[39m] = predictions.flatten()\n\u001b[32m     53\u001b[39m dummy_test[:, \u001b[32m0\u001b[39m] = y_test.flatten()\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m real_predictions = \u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43minverse_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdummy_pred\u001b[49m\u001b[43m)\u001b[49m[:, \u001b[32m0\u001b[39m]\n\u001b[32m     56\u001b[39m real_actuals = scaler.inverse_transform(dummy_test)[:, \u001b[32m0\u001b[39m]\n\u001b[32m     58\u001b[39m \u001b[38;5;66;03m# 6. Calculate Metrics\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adith\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:594\u001b[39m, in \u001b[36mMinMaxScaler.inverse_transform\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    584\u001b[39m xp, _ = get_namespace(X)\n\u001b[32m    586\u001b[39m X = check_array(\n\u001b[32m    587\u001b[39m     X,\n\u001b[32m    588\u001b[39m     copy=\u001b[38;5;28mself\u001b[39m.copy,\n\u001b[32m   (...)\u001b[39m\u001b[32m    591\u001b[39m     ensure_all_finite=\u001b[33m\"\u001b[39m\u001b[33mallow-nan\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    592\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m594\u001b[39m \u001b[43mX\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmin_\u001b[49m\n\u001b[32m    595\u001b[39m X /= \u001b[38;5;28mself\u001b[39m.scale_\n\u001b[32m    596\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m X\n",
      "\u001b[31mValueError\u001b[39m: operands could not be broadcast together with shapes (806,4) (2,) (806,4) "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "DATA_FILE = 'C:\\\\Users\\\\adith\\\\Downloads\\\\TradingProject\\\\data\\\\BTC_USD.csv'\n",
    "MODEL_FILE = 'C:\\\\Users\\\\adith\\\\Downloads\\\\TradingProject\\\\models\\\\market_model.h5'\n",
    "SCALER_FILE = 'C:\\\\Users\\\\adith\\\\Downloads\\\\TradingProject\\\\models\\\\market_scaler.pkl'\n",
    "LOOKBACK = 60\n",
    "TEST_SPLIT = 0.2\n",
    "\n",
    "print(\"ğŸš€ Loading Resources...\")\n",
    "\n",
    "# 1. Load Data & Scaler\n",
    "df = pd.read_csv(DATA_FILE)\n",
    "scaler = pickle.load(open(SCALER_FILE, \"rb\"))\n",
    "\n",
    "# Select Features (Must match training!)\n",
    "feature_cols = ['BTC_Close', 'BTC_Volume']\n",
    "dataset = df[feature_cols].values\n",
    "\n",
    "# Scale Data\n",
    "scaled_data = scaler.transform(dataset)\n",
    "\n",
    "# 2. Re-create Sequences (X, y)\n",
    "X, y = [], []\n",
    "for i in range(LOOKBACK, len(scaled_data)):\n",
    "    X.append(scaled_data[i-LOOKBACK:i]) \n",
    "    y.append(scaled_data[i, 0]) # Target: BTC Close\n",
    "    \n",
    "X, y = np.array(X), np.array(y)\n",
    "\n",
    "# 3. Split into Test Set (The last 20%)\n",
    "split_idx = int(len(X) * (1 - TEST_SPLIT))\n",
    "X_test = X[split_idx:]\n",
    "y_test = y[split_idx:]\n",
    "\n",
    "print(f\"ğŸ“Š Evaluating on {len(X_test)} unseen data points...\")\n",
    "\n",
    "# 4. Load Model & Predict\n",
    "model = load_model(MODEL_FILE)\n",
    "predictions = model.predict(X_test, verbose=0)\n",
    "\n",
    "# 5. Inverse Scale (Get back to Real $$$)\n",
    "# We need to create a dummy array because scaler expects 4 columns, but we have 1\n",
    "dummy_pred = np.zeros((len(predictions), 2))\n",
    "dummy_test = np.zeros((len(y_test), 2))\n",
    "\n",
    "dummy_pred[:, 0] = predictions.flatten()\n",
    "dummy_test[:, 0] = y_test.flatten()\n",
    "\n",
    "real_predictions = scaler.inverse_transform(dummy_pred)[:, 0]\n",
    "real_actuals = scaler.inverse_transform(dummy_test)[:, 0]\n",
    "\n",
    "# 6. Calculate Metrics\n",
    "rmse = np.sqrt(mean_squared_error(real_actuals, real_predictions))\n",
    "mae = mean_absolute_error(real_actuals, real_predictions)\n",
    "mape = np.mean(np.abs((real_actuals - real_predictions) / real_actuals)) * 100\n",
    "r2 = r2_score(real_actuals, real_predictions)\n",
    "\n",
    "# 7. Print Report\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"ğŸ§ª MODEL PERFORMANCE REPORT\")\n",
    "print(\"=\"*40)\n",
    "print(f\"ğŸ’° RMSE (Root Mean Sq Error):  ${rmse:,.2f}\")\n",
    "print(f\"ğŸ“‰ MAE (Mean Absolute Error): ${mae:,.2f}\")\n",
    "print(f\"ğŸ“Š MAPE (Percentage Error):   {mape:.2f}%\")\n",
    "print(f\"ğŸ“ˆ RÂ² Score (Accuracy Fit):   {r2:.4f} (Closer to 1.0 is best)\")\n",
    "print(\"=\"*40 + \"\\n\")\n",
    "\n",
    "# 8. Visual Plot\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(real_actuals, color='#f23645', label='Actual BTC Price', linewidth=1)\n",
    "plt.plot(real_predictions, color='#089981', label='AI Predicted Price', linewidth=1, linestyle='--')\n",
    "plt.title('Bitcoin Price Prediction (Test Data)')\n",
    "plt.xlabel('Time (Days in Test Set)')\n",
    "plt.ylabel('Price ($)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
